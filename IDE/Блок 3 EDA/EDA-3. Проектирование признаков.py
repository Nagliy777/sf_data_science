
#* В отличие от человека, модель машинного обучения не может проанализировать эти даты. А вот информация
#* о просрочке (разница между плановой и фактической датой погашения) представлена в числовом формате и
#* может сообщить некую информацию модели. Число, равное или меньше 0, говорит о клиенте как о благонадёжном
#* заёмщике; число больше нуля характеризует заёмщика как должника. Такое конструирование признаков помогает
#* дата-сайентисту не утратить часть данных, а также представить их в более «выгодном» свете.

#! Создание признаков

#todo Проектирование признаков — самый творческий процесс во всём цикле жизни создания модели машинного обучения.

#* Он включает в себя несколько этапов: 

#* создание признаков;
#* преобразование признаков;
#* отбор признаков. 

#todo Проектирование признаков необходимо для улучшения качества будущей модели: в процессе
#todo создания признаков появляются новые, возможно, более качественные, чем исходные.

#* Новые признаки могут быть сконструированы двумя способами: 

#* с помощью внешних источников данных;
#* из существующего набора данных.

#todo Пример 1

#* В колл-центрах часто используются системы автоматического подбора номеров для дозвона (рекомендация).
#* Если клиент во время прошлого созвона сбросил трубку, то автоматическая система должна снижать
#* рекомендацию этого номера. Клиент вряд ли ответит после того, как бросил трубку. 

#* В выделении этого факта нам поможет создание нового признака «сброс трубки» из информации в
#*  существующем датасете. Эта информация может быть получена из уже существующих признаков,
#*  например если время звонка составило меньше 10 секунд.

#todo Пример 2

#* Для определения дефолтности клиента банки делают запрос в БКИ с целью получения дополнительной
#* информации по клиенту. 

#* Такая информация о невыплаченных долгах другим организациям может быть решающим фактором в
#* вынесении решения по кредиту. Эту информацию можно получить с использованием внешних источников данных.

#todo Новые признаки можно сконструировать из уже существующего набора данных несколькими способами: 

#* разбор категорий; 
#* разбор даты и времени;
#* разбор числовых признаков;
#* разбор текста.

#? РАЗБОР ДАТЫ И ВРЕМЕНИ

#* Признаки, обозначающие дату и время, могут содержать много полезной информации. 

#* Например, в нашем датасете в названии вина можно увидеть год производства вина. Этот признак
#* может оказаться важным для определения рейтинга вина. Однако, пока он заключен в названии вина,
#* модель не сможет его использовать.

#todo Можно выделить год, месяц и часы в отдельные признаки: время суток, времена года и так далее.
#todo И если вы считаете, что ваш процесс зависит от времени, посвятите этому этапу как можно больше усилий.

#? РАЗБОР ЧИСЛОВЫХ ВЕЛИЧИН

#* В наборе данных очень часто встречаются числовые величины. Это могут быть вес, рост, расстояние и так далее. Вы можете преобразовывать их, создавать новые признаки, которые могут оказаться лучше исходных.
#* Например, в датасете винных обзоров числовым признаком является цена за бутылку вина. Вы можете
#*  округлить цену 35.363389$ в 35$, избавив данные от лишней информации.

#todo Создадим новый признак price_round, означающий округлённую до целого числа цену за бутылку вина:
# для удобства сразу преобразуем признак в int
data['price_round'] = data['price'].round().astype(int)

#* Для таких преобразований важен контекст. В контексте вина для сомелье цена в 35$ и 35.363389$ 
#* одинакова. Вы делаете округление признака, чтобы модель также не сосредотачивалась на сотых.
#* Это может помочь улучшить предсказание.

#todo Однако такое преобразование неуместно в ситуациях, когда сотые важны. Например, при предсказании
#todo курса лиры стоимость валюты в 6.12 руб. и в 6.84 руб. — абсолютно разные ситуации. 

#? РАЗБОР ТЕКСТА

#* Для того, чтобы извлечь из строки число, обозначающее год, необходимо воспользоваться регулярными выражениями.

#todo Регулярные выражения (regexp, или regex) — это механизм для поиска и замены текста.
#todo Это шаблоны, которые используются для поиска соответствующей части текста.

#*  Например, с помощью такого регулярного выражения [^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+ можно найти любой email в тексте.

#todo Выполните код для нахождения года вина при помощи регулярного выражения:
regex = '\d{4}' # регулярное выражение для нахождения чисел
data['year'] = data['title'].str.findall(regex).str.get(0)

#todo Разберём регулярное выражение \d{4}:

#* \d — класс символов, обозначает соответствие цифрам в диапазоне цифр [0-9];
#* {4} в шаблоне означает искать четыре вхождения символа, указанного ранее. В нашем случае это будут четырехзначные числа.
#* Таким образом, \d{4} означает поиск четырехзначных чисел в заданной строке.

#* Однако при поиске числа методом data['title'].str.findall(regex) результатом выполнения является
#* список найденных цифр. Поэтому необходимо извлечь первый элемент из списка найденных
#* методом str.get(0), где 0 — первый элемент в списке найденных чисел.

#todo Используя методы работы со строками pandas и регулярные выражения, можно извлечь любую информацию
#todo для новых признаков: даты, слова и выражения.

#? РАЗБОР КАТЕГОРИЙ

#* В наборе данных винных обзоров самая популярная страна-производитель вина — США. Возможно,
#*это не случайность, и факт производства в США влияет на рейтинг вина. Выделим этот факт.
#todo Вы можете создать новый бинарный признак is_usa и присвоить ему 1 в случае, если вино 
#todo произведено в США, иначе — 0.
data['is_usa'] = data['country'].apply(lambda x: 1 if x == 'US' else 0)
#* В наборе данных также есть ещё две страны, которые являются не менее популярными производителями вина.

#todo Возможно, производство вина в этих странах также влияет на рейтинг. Давайте создадим
#todo для них бинарные признаки.
data['is_france'] = data['country'].apply(lambda x: 1 if x == 'France' else 0)
data['is_italy'] = data['country'].apply(lambda x: 1 if x == 'Italy' else 0)

#* Создайте новый признак locality из признака title, который будет обозначать название долины/местности
#* производства вина.
#* Например, в названии вина Rainstorm 2013 Pinot Gris (Willamette Valley) locality будет
#* Willamette Valley. В названии Tandem 2011 Ars In Vitro Tempranillo-Merlot (Navarra) — Navarra.
#* Ответом на это задание является получившееся регулярное выражение и код преобразования.

regex = r'\((.+?)\)'
data['locality'] = data['title'].str.findall(regex).str.get(0)

#! Создание признаков. Внешние источники данных

#todo Внешние источники данных — дополнительные источники информации, использующиеся для
#todo обогащения датасета. Существует два типа внешних источников данных — открытые и закрытые.

#* Внешняя информация бывает общая и профильная.

#todo Общая внешняя информация — это различные общие географические, экологические,
#todo статистические и другие данные. 

#* географические: население страны, площадь страны, столица страны и так далее.
#* экологические: среднегодовая температура местности, текущее время года, уровень загрязнения воздуха и так далее.
#* статистические: доходы населения, половозрастной состав, уровень безработицы и так далее.
#* Они находятся в открытых источниках: Росстат, Википедия и так далее.

#* Для доступа к ним применяется парсинг, скачивание и работа с файлами файлов, реже — работа по API.

#todo Профильная внешняя информация — информация, связанная со сферой бизнеса, проблему которого необходимо решить. 
#* Например, при прогнозировании рейтинга ресторана мы можем использовать ссылки на сайт TripAdvisor
#* для парсинга и получения информации о ресторанах. При рекомендации фильмов в приложении мы можем
#* пользоваться информацией с «Кинопоиска» о фильмах.

#? РАБОТА С ФАЙЛАМИ

#* Часто маленькие страны с небольшим количеством населения имеют узкую специализацию. Например,
#* в производстве вина особенно успешны Франция, Италия, Испания, Новая Зеландия. Чтобы проверить,
#* влияет ли на качество вина населённость, выясним информацию о населении страны, в котором была
#* произведена бутылка вина. 

#todo Давайте прочтём его:

import pandas as pd
country_population = pd.read_csv('country_population.csv', sep=';')

country_population

#* Далее сопоставим значения из датасета country_population и страной-производителем вина. На
#* основе значений населения из country_population заполним новый признак country_population.
#* Используем для этого функцию для объединения датасетов join. Для объединения используем
#* аргумент on='country', указывая столбец, по которому объединяем датафреймы:

data=data.join(country_population.set_index('country'), on='country')

#* Итак, мы получили новый признак для нашего датасета — население страны.

#todo  Теперь используем файл country_area.zip (необходимо распаковать) для информации о площади страны.

#* Прочитаем файл:

country_area = pd.read_csv('country_area.csv', sep=';')
country_area

data=data.join(country_area.set_index('country'), on='country')


#todo Внешней информации можно получить очень много. Руководствуйтесь следующими правилами при работе с
#todo внешними источниками данных:

#* 1.Старайтесь найти профильную информацию по вашей бизнес-проблеме. При предсказании рейтинга фильма
#* информация об успехе режиссёра в других картинах будет полезнее, чем среднегодовая температура страны,
#* в которой снят фильм.

#* 2.Если у вас много времени и ресурсов, соберите как можно больше признаков любого качества. Чем
#* больше данных вы сгенерируете, тем выше вероятность получить качественные признаки на этапе отбора
#* данных.

#* 3.Используйте наименее трудозатратный для вас метод поиска информации: работа с файлами, парсинг,
#* запрос по API. Если останется время, можете попробовать другой подход.

#* 4.Не уделяйте всё время разработки поиску дополнительной информации. Вам ещё будет необходимо
#* построить модель, вывести её в продакшн — это также требует времени. 

#* 5.Помните, что цикл разработки модели цикличен: при достижении неудовлетворительных результатов
#* вы всегда сможете вернуться на любой шаг, в том числе и на шаг проектирования признаков.

#! Создание признаков. Работа с форматом «дата-время»

#todo Пример

#* В рекомендательных сервисах интернет-магазинов, которые тоже используют машинное обучение, очень
#* важно знать, когда клиент сделал последний заказ, когда клиент последний раз заходил на сайт,
#* когда последний раз просматривал ту или иную категорию товаров на сайте. На основе этой информации
#* модель определяет, какой товар лучше всего рекомендовать при следующем посещении сайта. Эта
#* Признак в таком формате не может быть передан на вход модели машинного обучения.

#todo Чтобы не потерять важную информацию, заключённую в формате даты и времени, необходимо
#todo преобразовать эту информацию таким образом, чтобы признак был в формате числа, а не строки. 

#* Например, дата последнего захода на сайт может быть преобразована в количество дней с момента
#* последнего входа на сайт, то есть результат разницы между текущей датой и датой последнего
#* захода на сайт:

#todo текущая дата - дата с последнего захода на сайт = количество дней с момента последнего
#todo посещения сайта 

#* Также мы можем посчитать количество минут, секунд, месяцев, лет с момента какой-либо даты.
#* Это зависит от контекста:

#* минуты и секунды актуальны для событий, часто меняющихся во времени;
#* месяцы и года — для событий, которые происходят реже. Например, вычисление возраста из даты
#* (лет), подсчёт стажа на последнем месте работы (месяцев, лет).

#* Создавая новые признаки из строковых признаков, мы также можем получить признаки в строковом
#* представлении как в случае со временем суток. Такой признак мы по-прежнему не можем передать
#* в модель. Но можно передать его в виде: 1 — утро, 2 — день, 3 — вечер, 4 — ночь. Этот приём
#* часто используется дата-сайентистами для оцифровки некоторых категорий и называется кодированием.
#* С кодированием признаков мы познакомимся в следующем юните.


#todo Давайте теперь приступим к практическим заданиям. В следующих заданиях мы будем использовать
#todo  срез базы данных из колл-центра. Компания хочет предсказывать, какому из клиентов стоит звонить
#todo  сегодня, а какому — нет.

#todo Давайте рассмотрим, из каких признаков состоит срез данных:

#* client_id — идентификатор клиента в базе;
#* agent_date — время соединения с агентом;
#* created_at — время соединения с клиентом (начало разговора);
#* end_date — время окончания соединения с клиентом (конец разговора).

#* Все признаки в наборе данных, за исключением номера клиента, представляют собой дату и время.
#*  Давайте создадим несколько признаков из этих данных.

#* Мы можем посчитать, сколько примерно длилось время разговора клиента и сотрудника компании
#*  — длительность разговора. 

#* Подсчитаем разницу между датой и временем начала разговора с клиентом и датой и временем окончания
#*  звонка.

calls['duration'] = (calls['end_date'] - calls['created_at']).dt.seconds

#* Таким образом мы получили новый признак duration — длительность разговора в секундах.

#todo Создайте новый признак is_connection — факт соединения с клиентом. Признак будет равен 1 в случае,
#todo если разговор состоялся и продлился больше 10 секунд, иначе — 0.

calls['is_connection']=calls['duration'].apply(lambda x: 1 if x > 10 else 0)

#todo Создайте признак time_diff — разницу в секундах между началом звонка(не разговора) и его окончанием.
calls['time_diff']=(calls['end_date']-calls['agent_date']).dt.seconds

#* так, мы получили четыре новых признака для нашего набора данных: duration, time_connection,
#* is_connection, time_diff. После генерации признаков из дат исходные признаки agent_date,
#* created_at, end_date нам больше не нужны — передать на вход модели мы им не сможем,
#* так как большинство моделей машинного обучения умеют работать только с числами, даты и текст ей
#* недоступны, поэтому удалим их:
calls = calls.drop(columns=['agent_date', 'created_at' ,'end_date'], axis=1)

#* Таким образом, мы получили набор данных с признаками, которые можно подать на вход модели,
#* и не потеряли важную информацию о событиях, произошедших в даты набора данных. 

#todo Создайте признак количество дней с момента произведения вина — years_diff для датасета винных обзоров.
#todo За дату отсчёта возьмите 12 января 2022 года. В ответ впишите максимальное количество дней с момента
#todo произведения вина. Ответ округлите до целого числа.

#* При попытке преобразовать созданный ранее столбец year в формат datetime, вы получите ошибку
#* OutOfBoundsDatetime, которая возникает из-за некорректного ожидаемого формата входных данных.
#* Чтобы справиться с этой ошибкой, воспользуйтесь параметром errors в функции to_datetime библиотеки
#* Pandas. Параметр нужно установить в значение coerce:
data['year'] = pd.to_datetime(data['year'], errors = 'coerce')
#* Подробнее о назначении этого параметра вы можете прочесть в документации.
data['years_diff']= pd.to_datetime('2022.01.12')- data['year']

#! Кодирование признаков. Методы
#* Ещё одним важным этапом проектирования признаков является обработка нечисловых (категориальных) признаков.
#* Многие модели машинного обучения не умеют работать с категориальными данными. Если мы передадим на вход
#* модели такие признаки, она выдаст ошибку. 

#todo Процесс преобразования категориальных данных в признаки, пригодные для обучения,
#todo называется кодированием признаков.

#todo Для кодирования категориальных признаков мы будем использовать библиотеку category_encoders.
#todo Это удобная библиотека для кодирования категориальных переменных различными методами.
#* Установим библиотеку:
pip install category_encoders

import category_encoders as ce

#todo Рассмотрим следующие популярные способы кодирования: 

#* порядковое кодирование (Ordinal Encoding); 
#* однократное кодирование (OneHot Encoding); 
#* бинарное кодирование (Binary Encoding).

#todo Создадим обучающий набор для кодирования порядковых признаков — ассортимент небольшого магазина с
#todo одеждой, где size — буквенное обозначение размера одежды, type — тип изделия.

import pandas as pd
# инициализируем информацию об одежде
clothing_list = [
    ['xxs', 'dress'],
    ['xxs', 'skirt'],
    ['xs', 'dress'],
    ['s', 'skirt'],
    ['m', 'dress'],
    ['l', 'shirt'],
    ['s', 'coat'],
    ['m', 'coat'],
    ['xxl', 'shirt'],
    ['l', 'dress']
]
clothing = pd.DataFrame(clothing_list, columns = ['size',  'type'])
clothing

#? ПОРЯДКОВОЕ КОДИРОВАНИЕ. ORDINAL ENCODING

#* В порядковой кодировке признаков каждому строковому значению присваивается значение в виде целого числа,
#* свойственного для конкретного значения строки.

#* Результат кодирования порядкового признака size будет выглядеть так: каждому строковому значению
#* присваивается значение в виде целого числа.

#* Выполним теперь кодирование порядкового признака size в Python. Порядковое кодирование в библиотеке
#* реализовано в классе OrdinalEncoder. По умолчанию все строковые столбцы будут закодированы.

#* Метод fit_transform устанавливает соответствия для кодирования и преобразовывает данные в
#* соответствие с ними. Затем используем метод concat() для добавления закодированного признака
#* в датафрейм data.

# импортируем библиотеку для работы с кодировщиками
import category_encoders as ce 

# создаем объект OrdinalEncoder, col - имя столбца, mapping - словарь с описанием кодировки
ord_encoder = ce.OrdinalEncoder(mapping=[{
	'col': 'size',
	'mapping': {'xxs': 1, 'xs': 2, 's': 3, 
                'm': 4, 'l': 5, 'xxl': 6}
}])
# применяем трансформацию к столбцу
data_bin = ord_encoder.fit_transform(clothing[['size']])
# добавляем результат к исходному DataFrame
clothing = pd.concat([clothing, data_bin], axis=1)

#* Порядковое кодирование плохо работает для номинальных признаков. Ошибку при кодировании мы не получим,
#* но алгоритмы машинного обучения не могут различать категории и числовые признаки, поэтому могут быть
#* сделаны выводы о неправильном порядке. 

#* Порядковое кодирование может успешно использоваться для кодирования порядковых признаков.
#* Мы можем закодировать признак size — размер одежды со значениями xxs, xs, s соответственно в 
#* значения 1, 2, 3. Это будет логично, и моделью не будут сделаны выводы о неправильном порядке.
#* Увеличение размера будет соответствовать логическому увеличению кода этого значения: xxs меньше
#* xs, и числовой код 1 (xxs) меньше, чем числовой код 2 (xs).

#* В случае с магазином одежды для размера одежды size уместно применить порядковое кодирование.
#* Кодирование размера xxs, xs, s в 1, 2, 3 будет соответствовать логическому увеличению порядка.
#* А для номинального признака type необходимо подобрать другое кодирование. Мы сделаем это далее в
#* юните при знакомстве с OneHot-кодированием.

#todo Используйте ранее изученные методы кодирования и закодируйте признак year в датасете винных обзоров
#todo порядковым кодированием.

oe = ce.OrdinalEncoder(cols=['year'])
year_coded = oe.fit_transform(data['year'].sort_values(ascending=True)) # кодируем отсортированный список года производства
pd.concat([data.sort_values(by='year', ascending=True), year_coded], axis=1) # присоединяем к отсортированному DF список с порядковыми категориями

#? ОДНОКРАТНОЕ КОДИРОВАНИЕ. ONE-HOT ENCODING

#* Однократное кодирование (его ещё часто называют «горячим») является автоматизированным кодированием,
#* которое мы делали в юните Создание признаков. Для каждой новой категории создается новый бинарный признак.
#* Значение 1 в этих признаках проставляется там, где значение исходного признака равно этой категории.

#todo Этот способ кодирования понятен, хорошо работает как на номинальных, так и на порядковых признаках.
#todo Однако существует один минус: количество созданных признаков равно количеству уникальных значений
#todo категориального признака. 
 
#todo В нашем примере с признаком color мы получили три новых признака color_red color_blue, color_green.
#todo Но представьте, что в наборе данных у нас попадётся признак с большим количеством категорий.

#todo Закодируем признак type в Python. Используем класс OneHotEncoding библиотеки category_encoders.
#todo Укажем в cols наименование признака type для кодировки, иначе будут закодированы все строковые столбцы.

import category_encoders as ce # импорт для работы с кодировщиком

encoder = ce.OneHotEncoder(cols=['type'], use_cat_names=True) # указываем столбец для кодирования
type_bin = encoder.fit_transform(clothing['type'])
clothing = pd.concat([clothing, type_bin], axis=1)

#* Таким образом, мы получили четыре новых признака для категорий coat, dress, shirt, skirt.В строке нужного
#* типа исходного признака стоит значение 1, в остальных строках — 0. Эти признаки пригодны для обучения.

#todo На самом деле метод однократного кодирования реализован в pandas в функции pd.get_dummies().
#todo Для выполнения кодирования достаточно передать в функцию DataFrame и указать столбцы, для которых
#todo должно выполняться кодирование. По умолчанию кодирование выполняется для всех столбцов типа object:

clothing_dummies = pd.get_dummies(clothing, columns=['type'])
#todo Новые бинарные признаки также часто называются dummy-признаками или dummy-переменными.  

#? ДВОИЧНОЕ КОДИРОВАНИЕ

#* Принцип двоичного кодирования похож на однократное кодирование, но создаёт меньше столбцов.
#* При однократном кодировании признака с количеством уникальных категорий 100 шт. мы создадим
#* 100 новых признаков, а при двоичном кодирования мы сгенерируем всего 7 признаков.

#todo 1. Сначала признак кодируется в числовое представление, как мы делали это при кодировании порядковых
#todo признаков: hot — 1, cold — 2, … и так далее.

#todo 2. Затем каждое числовое представление, выраженное целым числом, переводится в двоичный
#todo код: 1 – 001, 2 – 010, 3 – 011,... и так далее.

#todo 3. Затем для каждого двоичного представления создаются новые признаки. В нашем случае двоичное
#todo представления уместилось в три числа, поэтому итогом стало создание трёх новых признаков.

#todo Пошаговый алгоритм двоичного кодирования можно описать так: 

#* значения признака кодируются в некоторый числовой порядок;
#* целые числа кодируются в двоичный код;
#* цифры двоичного представления формируют новые столбцы.

#todo Пример

#* Вернёмся к примеру с магазином одежды. Закодируем бинарным способом признак type в Python.
#* Используем класс BinaryEncoder библиотеки category_encoders.

import category_encoders as ce # импорт для работы с кодировщиком
bin_encoder = ce.BinaryEncoder(cols=['type']) # указываем столбец для кодирования
type_bin = bin_encoder.fit_transform(clothing['type'])
clothing = pd.concat([clothing, type_bin], axis=1)

#* Метод однократного кодирования можно использовать почти во всех сценариях подготовки данных к обучению
#* за исключением некоторых алгоритмов и проблем с памятью. О том, почему из-за этого может возникнуть
#* проблема, вы узнаете подробнее в модулях про машинное обучение. 

#* В случаях проблем с памятью необходимо обратиться к другим кодировщикам — к порядковому, бинарному
#* кодировщику или иным.

#! Преобразование признаков. Нормализация. Стандартизация

#* Проектирование признаков подразумевает под собой изменение данных, их удаление, создание и тд.
#* В этом юните мы изучим основы преобразования признаков и реализацию этих методов в Python.

#todo В машинном обучении данные часто подвергают различным преобразованиям. Самые популярные
#todo из них — это нормализация и стандартизация, которые мы рассмотрим в этом юните.
#todo Примечание. Операции нормализации и стандартизации также часто называют шкалированием

#todoПроблема

#* Часто данные для обучения представлены в различных единицах измерения, в разном масштабе. Например,
#* в наборе данных может быть представлен признак кадастровая стоимость недвижимости, которая измеряется
#* в миллионах рублей, и признак понижающий коэффициент, который используется для определения налога на
#* недвижимость и измеряется в сотых долях. Или, например, в одном наборе данных встречается признак доход
#* клиента и стаж работы на последнем месте. Стаж в редких случаях поднимается выше 10, тогда как доход
#* измеряется тысячами.

#* Таким образом, признаки, которые измеряются в разных масштабах, в разной степени влияют на предсказание
#* модели. Для решения этой проблемы перед обучением обычно делают преобразование признаков.
#* Рассмотрим такие преобразования, как нормализация и стандартизация.

#? НОРМАЛИЗАЦИЯ

#todo Нормализация — один из методов преобразования входных признаков, при котором значения признаков
#todo приводятся к заданному диапазону, например [0,...,1]. 

#* Существует несколько способов нормализации: MinMaxScaler, RobustScaler.


#todo Используем библиотеку numpy для создания массивов случайных чисел различных распределений.
#todo Выполните этот код, чтобы создать обучающий набор различных распределений:

import numpy as np 
import pandas as pd

np.random.seed(34)

# для нормализации, стандартизации
from sklearn import preprocessing

# Для графиков
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
matplotlib.style.use('ggplot')

# сгенерируем датасет из случайных чисел
df = pd.DataFrame({ 
    # Бета распределение, 5 – значение альфа, 1 – значение бета, 1000 – размер
    'beta': np.random.beta(5, 1, 1000) * 60,
    
    # Экспоненциальное распределение, 10 – "резкость" экспоненты, 1000 – размер
    'exponential': np.random.exponential(10, 1000),
    
    # Нормальное распределение, 10 – среднее значение р., 2 – стандартное отклонение, 1000 – количество сэмплов
    'normal_p': np.random.normal(10, 2, 1000),
    
    # Нормальное распределение, 10 – среднее значение р., 10 – стандартное отклонение, 1000 – количество сэмплов
    'normal_l': np.random.normal(10, 10, 1000),
})

# Копируем названия столбцов, которые теряются при использовании fit_transform()
col_names = list(df.columns)

#* Сгенерированные распределения выбраны случайным образом, однако вы можете встретить их, например,
#* в таких наборах данных:

#todo Бета-распределение моделирует вероятность. Например, коэффициент конверсии клиентов, купивших что-то
#todo на сайте.

#todo Экспоненциальное распределение, предсказывающее периоды времени между событиями. Например, время ожидания
#todo автобуса.

#todo Нормальное распределение, например распределение роста и веса человека.

#* Рассмотрим распределения на графике. Метод визуализации kdeplot() — это метод визуализации распределения
#* наблюдений в наборе данных. Он представляет собой непрерывную линию плотности вероятности. Подробнее об
#* этой функции вы можете прочитать в руководстве.

# зададим параметры холста, название и визуализируем кривые распределения:
fig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))
ax1.set_title('Исходные распределения')

# kdeplot() (KDE – оценка плотности ядра) – специальный метод для графиков распределений
sns.kdeplot(df['beta'], ax=ax1, label ='beta')
sns.kdeplot(df['exponential'], ax=ax1, label ='exponential')
sns.kdeplot(df['normal_p'], ax=ax1, label ='normal_p')
sns.kdeplot(df['normal_l'], ax=ax1, label ='normal_l')
plt.legend()

#todo Зафиксируем описательные статистики до преобразований.

df.describe()

#? MINMAXSCALER

#* При применении нормализации методом MinMaxScaler все значения признаков будут преобразованы в
#* диапазон [0,1], что означает, что минимальное и максимальное значение признака будет равно 0 и 1
#* соответственно.

#todo Нормализация происходит следующим способом:

#* Из каждого значения признака х вычитается минимальное значение этого признака
#* Результат вычитания делится на разность между максимумом и минимумом признака

#todo Например, температура в горном посёлке за день может меняться от 10 до 35 градусов.
#todo Текущая температура составляет 17 градусов. 
Xscaled=(17-10)/(35-10)

#* Нормализованное значение 0.28 лежит в диапазоне от 0 до 1, и ближе к левой границе распределения (0),
#* что соответствует также ненормализованному распределению (значение 17 ближе к 10).

#todo Класс MinMaxScaler делает вышеописанную нормализацию автоматически при помощи функции преобразования
#todo fit_transform. Вы познакомитесь с ней подробнее в модулях машинного обучения.

# инициализируем нормализатор MinMaxScaler
mm_scaler = preprocessing.MinMaxScaler()

# кодируем исходный датасет
df_mm = mm_scaler.fit_transform(df)

# Преобразуем промежуточный датасет в полноценный датафрейм для визуализации
df_mm = pd.DataFrame(df_mm, columns=col_names)

fig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))
ax1.set_title('После нормализации MinMaxScaler')

sns.kdeplot(df_mm['beta'], ax=ax1)
sns.kdeplot(df_mm['exponential'], ax=ax1)
sns.kdeplot(df_mm['normal_p'], ax=ax1)
sns.kdeplot(df_mm['normal_l'], ax=ax1)

#* Как мы видим, значения расположились в диапазоне от минимального 0 до максимального 1,
#* как и должно произойти при таком способе нормализации. Также сохранилась форма исходного распределения:
#* скошенные влево/вправо распределения сохранили свою форму.

#? ROBUSTSCALER

#todo Нормализация методом RobustScaler происходит в несколько этапов:

#* из каждого значения признака вычитается медиана признака
#* полученное значение делится на межквартильный размах

#todo Например, имеется числовой ряд [1, 2, 3, 4, 5]. Медиана ряда — 3.
#todo Межквартильный размах: 4-2=2 Мы хотим нормализовать число 4.
Xscaled=(4-2)/2=0.5
#* Таким образом, значение 4 после нормализации приняло значение 0.5.

#todo Проведём нормализацию распределений признаков из обучающего примера, используя класс RobustScaler.

# инициализируем нормализатор RobustScaler
r_scaler = preprocessing.RobustScaler()

# кодируем исходный датасет
df_r = r_scaler.fit_transform(df)

# Преобразуем промежуточный датасет в полноценный датафрейм для визуализации
df_r = pd.DataFrame(df_r, columns=col_names)

fig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))
ax1.set_title('Распределения после RobustScaler')

sns.kdeplot(df_r['beta'], ax=ax1)
sns.kdeplot(df_r['exponential'], ax=ax1)
sns.kdeplot(df_r['normal_p'], ax=ax1)
sns.kdeplot(df_r['normal_l'], ax=ax1)

#* Из описательных статистик видно, что RobustScaler не масштабирует данные в заданный интервал, как
#* делает это MinMaxScaler. Однако распределения не сохранили своё исходное состояние. Левый хвост
#* экспоненциального распределения стал практически незаметным. То же произошло и с бета-распределением.
#* Они стали более нормальными.

#todo В ЧЁМ РАЗНИЦА?

#* Поскольку MinMaxScaler использует в своём вычислении минимальное и максимальное значения признака,
#* то этот способ будет чувствителен к наличию выбросов в распределении.

#* RobustScaler в вычислении использует перцентили, и поэтому не зависит даже от большого количества выбросов.

#todo Поэтому, если ваши данные уже были очищены от выбросов, смело используйте MinMaxScaler. Этот алгоритм
#todo нормализации используется специалистами по данным чаще всего в силу его простоты и понятности, а данные
#todo на этом этапе чаще всего уже очищены. Если вы пропустили этап очистки данных и нормализуете признаки
#todo раньше, используйте нормализатор RobustScaler.

#? СТАНДАРТИЗАЦИЯ

#todo Стандартизация — ещё один метод преобразования входных признаков, при котором изменяется 
#todo распределение таким образом, чтобы среднее значений равнялось 0, а стандартное отклонение — 1. 

#* Например, у нас есть числовой ряд [1, 2, 3, 4, 5]. Среднее ряда: 3.
#* Стандартное отклонение — 1.4. Нормализуем число 4.
(4-3)/1.4=0.7
#* Нормализованное число 4 равно 0.7.

#todo Этот процесс можно описать как центрирование данных с масштабированием. Сначала происходит
#todo вычитание среднего значения из всех данных — центрирование,  а затем деление на отклонение. 

#? ДЛЯ ЧЕГО НЕОБХОДИМО ТАКОЕ ПРЕОБРАЗОВАНИЕ?

#todo Как и нормализация, стандартизация может быть полезна при данных с разными масштабами.
#todo Однако в отличие от нормализации стандартизация предполагает, что признак распределён нормально.

#* Чтобы понять, как стандартизация меняет распределение, рассмотрим метод стандартизации
#* StandardScaler в Python

#? STANDARDSCALER

#todo Для стандартизации используем класс StandardScaler.

# инициализируем стандартизатор StandardScaler
s_scaler = preprocessing.StandardScaler()

# кодируем исходный датасет
df_s = s_scaler.fit_transform(df)

# Преобразуем промежуточный датасет в полноценный датафрейм для визуализации
df_s = pd.DataFrame(df_s, columns=col_names)

fig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))
ax1.set_title('Распределения после StandardScaler')

sns.kdeplot(df_s['beta'], ax=ax1)
sns.kdeplot(df_s['exponential'], ax=ax1)
sns.kdeplot(df_s['normal_p'], ax=ax1)
sns.kdeplot(df_s['normal_l'], ax=ax1)

#* Стандартное отклонение стало равным 1, а средние значения — 0. Распределения изменились подобно
#* RobustScaler — стали более нормальными.

#? НОРМАЛИЗОВАТЬ ИЛИ СТАНДАРТИЗИРОВАТЬ?

#todo если признак распределён нормально, то его необходимо стандартизировать;
#todo если признак распределён ненормально, его необходимо нормализовать;
#todo если разброс значений небольшой, то можно обойтись без преобразования данных.

#! Отбор признаков. Мультиколлинеарность

#? ДЛЯ ЧЕГО НЕОБХОДИМО ОТБИРАТЬ ПРИЗНАКИ?

#* Сократить время обучения. Чем меньше данных, тем быстрее обучается модель. Например,
#* в скоринговых моделях часто количество признаков на этапе проектирования составляет больше
#* 500, и дата-сайентисты делают отбор признаков, чтобы исключить те признаки, которые вносят
#* наименьший вклад. В редких случаях количество признаков в модели может быть больше 100.

#* Повысить качество предсказания. Избыточные данные могут снижать точность предсказания, могут
#* выступать в качестве «шума». Это явление называют мультиколлинеарностью, которую мы изучим ниже.
#* Мы уже говорили о «проклятии размерности» в юните Работа с пропусками: методы обработки.

#? КАК ПОНЯТЬ, КАКИЕ ПРИЗНАКИ БОЛЬШЕ ВСЕГО ВЛИЯЮТ НА ПРЕДСКАЗАНИЕ?

#todo Прочитаем датасет и посмотрим на первые несколько строк.

import pandas as pd

iris = pd.read_csv('iris.csv')
iris.head()

#todo Проведём корреляционный анализ датасета и используем для этого тепловую карту
#todo корреляций признаков.

# импортируем seaborn для построения графиков
import seaborn as sns

# отсеиваем числовые признаки и включаем отображение коэффициентов
sns.heatmap(iris.corr(numeric_only=True), annot=True)

#* Чтобы построить модель на этом наборе данных, сделаем отбор признаков — удалим признаки
#* с очень сильной корреляцией (где коэффициент корреляции +/-0.7 и выше).

iris = iris.drop(['petal.width'], axis=1)
iris = iris.drop(['petal.length'], axis=1)

#* У нас осталось всего два признака с коэффициентом корреляции -0.12: sepal.width и sepal.length,
#* и признак, который необходимо предсказать — variety. Связь между оставшимися признаками очень
#* слабая, поэтому эти признаки будут включены в итоговый набор данных для обучения.

#? КАКОЙ ПРИЗНАК УДАЛЯТЬ?

#todo Как понять, какой признак необходимо удалить из пары скоррелированных признаков?

#* Вы можете удалить любой признак из пары. Однако вы можете оставить в наборе данных тот признак,
#* который легче будет использовать в дальнейшем. Например, для него не понадобится округление или
#* нормализация. с целым числом удобнее работать.

#todo Практика

#*Создайте новый признак old, где 1 — при возрасте пациента более 60 лет.
heart['old']=heart['age'].apply(lambda x: 1 if x >60 else 0)

#* Создайте новый признак trestbps_mean, который будет обозначать норму давления в среднем для его возраста
#* и пола. trestbps — систолическое артериальное давление в состоянии покоя.
def get_trestbps_mean(arg):  
    if arg['sex'] == 1:
        if arg['age'] <= 20: return '123/76'
        if 21 <=arg['age'] <= 30: return '126/79'   
        if 31 <=arg['age'] <= 40: return '129/81'   
        if 41 <=arg['age'] <= 50: return '135/83'  
        if 51 <=arg['age'] <= 60: return '142/85'  
        if 61 <=arg['age']: return '142/80'
    if arg['sex'] == 0:
        if arg['age'] <= 20: return '116/72'
        if 21 <=arg['age'] <= 30: return '120/75'   
        if 31 <=arg['age'] <= 40: return '127/80'   
        if 41 <=arg['age'] <= 50: return '137/84'  
        if 51 <=arg['age'] <= 60: return '144/85'  
        if 61 <=arg['age']: return '159/85'

    
heart['trestbps_mean'] = heart[['age', 'sex']].apply(get_trestbps_mean, axis=1)

#* Закодируйте вышеперечисленные признаки методом OneHotEncoding из библиотеки Category Encoders,
#* удалив исходные признаки. Параметр use_cat_names оставьте по умолчанию.

import category_encoders as ce # импорт для работы с кодировщиком

encoder = ce.OneHotEncoder(cols=['cp', 'restecg', 'slope', 'ca', 'thal'], use_cat_names=True) # указываем столбец для кодирования
type_bin = encoder.fit_transform(heart[['cp', 'restecg', 'slope', 'ca', 'thal']])
heart = pd.concat([heart, type_bin], axis=1)

heart.drop(['cp', 'restecg', 'slope', 'ca', 'thal'], axis=1) # удаление признаков

#* Нормализуйте все числовые признаки подходящим способом.

# для нормализации, стандартизации
from sklearn import preprocessing
# Копируем названия столбцов, которые теряются при использовании fit_transform()
col_names = list(heart.columns)
# инициализируем нормализатор RobustScaler
r_scaler = preprocessing.RobustScaler()

# кодируем исходный датасет
df_heart = r_scaler.fit_transform(heart)
heart = pd.DataFrame(df_heart, columns=col_names)