
#! Сортировка данных в DataFrame
#todo  Для сортировки значений в DataFrame по значениям одного или
#todo  нескольких столбцов используется метод sort_values().

#? by — имя или список имён столбцов, по значениям которых производится сортировка.

#? axis — ось, по которой производится сортировка (0 — строки, 1 — столбцы). 
#? По умолчанию сортировка производится по строкам.

#? ascending — сортировка по возрастанию (от меньшего к большему).
#? По умолчанию параметр выставлен на True, для сортировки по убыванию (от большего к меньшему)
#? необходимо выставить его на False.

#? ignore_index — создаются ли новые индексы в таблице. По умолчанию выставлен на False
#? и сохраняет индексы изначальной таблицы.

#? inplace — производится ли замена исходной таблицы на отсортированную. По умолчанию параметр
#? выставлен на False, то есть замены не производится. Чтобы переопределить исходную таблицу
#? на отсортированную, необходимо выставить этот параметр на True.

#todo СОРТИРОВКА ПО ЗНАЧЕНИЯМ ОДНОГО СТОЛБЦА

melb_df.sort_values(by='Price').head(10)

#*А теперь отсортируем таблицу по убыванию (от самой последней до самой первой)
#*даты продажи объекта (Date). Для этого выставим параметр ascending на False:

melb_df.sort_values(by='Date', ascending=False)

#todo СОРТИРОВКА ПО ЗНАЧЕНИЯМ НЕСКОЛЬКИХ СТОЛБЦОВ
#* Для сортировки по значениям нескольких столбцов необходимо передать названия этих
#* столбцов в параметр by в виде списка. При этом важно обращать внимание на порядок следования столбцов.
#* Так, например, отсортируем таблицу сначала по возрастанию расстояния от центра города (Distance),
#* а затем — по возрастанию цены объекта (Price). Для того чтобы вывод был более наглядным,
#* выделим каждую десятую строку из столбцов Distance и Price результирующей таблицы:

melb_df.sort_values(by=['Distance', 'Price']).loc[::10, ['Distance', 'Price']]

#todo КОМБИНИРОВАНИЕ СОРТИРОВКИ С ФИЛЬТРАЦИЕЙ

#* Найдём информацию о таунхаусах (Type), проданных компанией (SellerG) McGrath, у которых коэффициент
#* соотношения площадей здания и участка (AreaRatio) меньше -0.8. Результат отсортируем по дате
#* продажи (Date) в порядке возрастания, а после проведём сортировку по убыванию коэффициента
#* соотношения площадей. Также обновим старые индексы на новые, установив параметр ignore_index на True.
#* Для наглядности результата выберем из таблицы только столбцы Data и AreaRatio:

mask1 = melb_df['AreaRatio'] < -0.8
mask2 = melb_df['Type'] == 'townhouse'
mask3 = melb_df['SellerG'] == 'McGrath'
melb_df[mask1 & mask2 & mask3].sort_values(
    by=['Date', 'AreaRatio'],
    ascending=[True, False],
    ignore_index=True
).loc[:, ['Date', 'AreaRatio']]

#* Примечание. Старайтесь не сочетать фильтрацию и метод sort_values()
#* с параметром inplace=True, так как в таком случае у вас возникнет предупреждение
#* что это не ошибка и код в таком случае отработает. Однако Pandas предупреждает вас о
#* том, что при использовании такого кода дальнейшие результаты могут быть неожиданными.
#*Чтобы не возникало подобных конфликтов, необходимо использовать метод copy() для явного создания копии 
filtered = melb_df[melb_df['Rooms'] > 5].copy()

#! Группировка данных в DataFrame

#* Одна из основных задач анализа данных — это группировка данных и сравнение показателей в группах.

#? МЕТОД GROUPBY()
#* В библиотеке Pandas для группировки данных по одному или нескольким признакам
#* можно использовать метод groupby()
#?Основные параметры метода groupby()
#? by — имя или список имён столбцов, по которым производится группировка.
#? axis — ось, по которой производится группировка (0 — строки, 1 — столбцы). По умолчанию группировка производится по строкам.
#? as_index — добавляется ли дополнительный индекс к таблице. По умолчанию установлен на True.
#todo Применим агрегирующую функцию среднего к результату работы groupby(). 
#todo В качестве столбца для группировки возьмём столбец типа объекта недвижимости (Type):

melb_df.groupby(by='Type').mean(numeric_only=True)

#* Мы получили таблицу, на пересечении строк и столбцов которой находятся средние
#* значения каждого числового признака в наших данных.
#*Обратите внимание на структуру получившейся таблицы: теперь на месте индексов стоят
#*значения типа объекта недвижимости Type (house, townhouse, unit).
#* Если мы хотим видеть тип объекта в качестве отдельного столбца таблицы,
#* мы можем выставить параметр as_index на False

#* Как правило, нам не нужна информация обо всех столбцах, поэтому агрегирующие методы
#* можно применять только к интересующему нас столбцу.
melb_df.groupby('Type')['Price'].mean()

#todo найдём минимальное значение расстояния от центра города до объекта
#todo в зависимости от его региона. Результат отсортируем по убыванию расстояния:
melb_df.groupby('Regionname')['Distance'].min().sort_values(ascending=False)

#! ГРУППИРОВКА ДАННЫХ ПО ОДНОМУ КРИТЕРИЮ С НЕСКОЛЬКИМИ АГРЕГАЦИЯМИ
#todo Чтобы рассчитать несколько агрегирующих методов, можно воспользоваться методом agg(),
#todo который принимает список строк с названиями агрегаций.
#* Давайте построим таблицу для анализа продаж по месяцам. Для этого найдём количество продаж,
#* а также среднее и максимальное значения цен объектов недвижимости (Price), сгруппированных
#* по номеру месяца продажи (MonthSale). Результат отсортируем по количеству продаж в порядке убывания:

melb_df.groupby('MonthSale')['Price'].agg(
    ['count', 'mean', 'max']
).sort_values(by='count', ascending=False)

#* Примечание. Если вам нужна полная информация обо всех основных статистических
#* характеристиках внутри каждой группы, вы можете воспользоваться методом agg(),
#* передав в качестве его параметра строку 'describe':

melb_df.groupby('MonthSale')['Price'].agg('describe')

#* Так, например, мы можем вычислить число уникальных риелторских компаний в зависимости
#* от региона, чтобы понять, в каких регионах конкуренция на рынке недвижимости меньше. Это
#* можно сделать, передав в параметр метода agg() строку 'nunique'. Более того, метод agg() поддерживает 
#* использование и других функций. Передадим дополнительно встроенную функцию set, чтобы получить множество
#*из агентств недвижимости, которые работают в каждом из регионов:
 
melb_df.groupby('Regionname')['SellerG'].agg(
    		['nunique', set]
      )

#! Сводные таблицы
#* Сводная таблица принимает на вход данные из отдельных столбцов и группирует их.
#* В результате получается новая таблица, которая позволяет увидеть многомерное обобщение
#* данных. Таким образом, благодаря сводным таблицам мы можем оценить зависимость между двумя
#* и более признаками данных.

#* Также можно построить таблицу, в которой мы будем учитывать не только число комнат, но и тип
#* здания (Type). Для этого в параметрах метода groupby() укажем список из нескольких интересующих
#* нас столбцов.
melb_df.groupby(['Rooms', 'Type'])['Price'].mean()


#* В результате выполнения такого кода мы получаем Series, которая обладает несколькими уровнями индексов:
#* первый уровень — число комнат, второй уровень — тип здания. Такая организация индексов называется иерархической

#todo Для того, чтобы финальный результат был представлен в виде сводной таблицы (первый группировочный
#todo признак по строкам, а второй — по столбцам), а не в виде Series с иерархическими индексами,
#todo к результату чаще всего применяют метод unstack(), который позволяет переопределить вложенный индекс в виде столбцов таблицы:
melb_df.groupby(['Rooms', 'Type'])['Price'].mean().unstack()

#! МЕТОД PIVOT_TABLE ДЛЯ ПОСТРОЕНИЯ СВОДНЫХ ТАБЛИЦ
#* На самом деле метод groupby редко используется при двух параметрах, так как для
#* построения сводных таблиц существует специальный и более простой метод — pivot_table().

#? Основные параметры метода pivot_table()
#? values — имя столбца, по которому необходимо получить сводные данные, применяя агрегирующую функцию;
#? index — имя столбца, значения которого станут строками сводной таблицы;
#? columns — имя столбца, значения которого станут столбцами сводной таблицы;
#? aggfunc — имя или список имён агрегирующих функций (по умолчанию — подсчёт среднего, 'mean');
#? fill_value — значение, которым необходимо заполнить пропуски (по умолчанию пропуски не заполняются).

#* Давайте построим ту же самую таблицу, но уже с использованием метода pivot_table.
#* В качестве параметра values укажем столбец Price, в качестве индексов сводной таблицы возьмём Rooms,
#* а в качестве столбцов — Type

melb_df.pivot_table(
    values='Price',
    index='Rooms',
    columns='Type',
    fill_value=0
).round()

#* А теперь давайте проанализируем продажи в каждом из регионов в зависимости от того,
#* будний был день или выходной.
melb_df.pivot_table(
    values='Price',
    index='Regionname',
    columns='Weekend',
    aggfunc='count'
)

#* Разберём ещё один пример: найдём, как зависит средняя и медианная площадь участка (Landsize)
#* от типа объекта (Type) и его региона (Regionname). Чтобы посмотреть несколько статистических
#* параметров, нужно передать в аргумент aggfunc список из агрегирующих функций.
melb_df.pivot_table(
    values='Landsize',
    index='Regionname',
    columns='Type',
    aggfunc=['median', 'mean'],
    fill_value=0
)

#* Обратите внимание на добавление дополнительных индексов столбцов median и mean. Здесь медианное
#* и среднее значения рассчитаны отдельно для каждой комбинации признаков.

#! МНОГОМЕРНЫЕ СВОДНЫЕ ТАБЛИЦЫ
#* Сводные таблицы позволяют наблюдать зависимость и от большего числа признаков.
#* Такие сводные таблицы называются многомерными. 

#todo Давайте построим таблицу, в которой по индексам будут располагаться признаки метода продажи
#todo (Method) и типа объекта (Type), по столбцам — наименование региона (Regionname), а на пересечении
#todo строк и столбцов будет стоять медианная цена объекта (Price):
melb_df.pivot_table(
    values='Price',
    index=['Method','Type'],
    columns='Regionname',
    aggfunc='median',
    fill_value=0
)

#! ДОСТУП К ДАННЫМ В СВОДНОЙ ТАБЛИЦЕ

#* Запишем сводную таблицу, которую мы создавали ранее в переменную pivot:
pivot = melb_df.pivot_table(
    values='Landsize',
    index='Regionname',
    columns='Type',
    aggfunc=['median', 'mean'],
    fill_value=0
)

pivot.columns

#* В результате мы получаем объект MultiIndex. Этот объект хранит в себе шесть комбинаций пар 
#* столбцов (два статистических параметра и три типа здания)
#todo Так, из таблицы pivot мы можем получить средние значения площадей участков для типа здания unit,
#todo просто последовательно обратившись по имени столбцов:
display(pivot['mean']['unit'])

#* Аналогично производится и фильтрация данных. Например, если нам нужны регионы, в которых 
#* средняя площадь здания для домов типа house меньше их медианной площади, то мы можем
#* найти их следующим образом:
mask = pivot['mean']['house'] < pivot['median']['house']
filtered_pivot = pivot[mask]
display(filtered_pivot)

#* Чтобы получить индексы отфильтрованной таблицы, можно воспользоваться
#* атрибутом index и обернуть результат в список:
print(list(filtered_pivot.index))
# ['Southern Metropolitan', 'Western Metropolitan']

#* Примечание. На самом деле мультииндексные таблицы можно создавать и вручную.
#* Давайте посмотрим на синтаксис данной конструкции:
import numpy as np
mser = pd.Series(
    np.random.rand(8),
	index=[['white','white','white','blue','blue','red','red','red'], 
           ['up','down','right','up','down','up','down','left']])
display(mser)

#* В данном примере мы создаём объект Series со вложенными индексами. Мы передаём в качестве индексов
#* Series вложенный список, где первый список задаёт внешний уровень вложенности, а второй список —
#* внутренний уровень вложенности. Значения Series — случайные числа от 0 до 1, сгенерированные функцией 
#* np.random.rand() (ваши значения могут отличаться).

#! Объединение DataFrame: знакомимся с новыми данными
#* Следуя нашему плану объединения таблиц, первым делом
#* мы должны склеить таблицы ratings1 и ratings2 по строкам.
#* Для этого воспользуемся встроенной функцией Pandas concat(), которая позволяет
#* склеивать (конкатенировать) таблицы как по строкам, так и по столбцам.
#? Основные параметры функции concat()
#? objs — список объектов DataFrame ([df1, df2,…]), которые должны быть сконкатенированы;
#? axis — ось определяет направление конкатенации: 0 — конкатенация по строкам (по умолчанию), 1 — конкатенация по столбцам;
#? join — либо inner (пересечение), либо outer (объединение); рассмотрим этот момент немного позже;

#? ignore_index — по умолчанию установлено значение False, которое позволяет значениям индекса
#?оставаться такими, какими они были в исходных данных. Если установлено значение True, параметр
#?будет игнорировать исходные значения и повторно назначать значения индекса в последовательном порядке.

#* Для корректной конкатенации по строкам объединяемые таблицы должны
#* иметь одинаковую структуру — идентичное число и имена столбцов.

#todo Итак, давайте склеим  ratings1 и ratings2 по строкам, так как они имеют одинаковую структуру столбцов.
#todo Для этого передадим их списком в функцию concat(). Помним, что параметр axis по умолчанию равен 0,
#todo объединение происходит по строкам, поэтому не трогаем его. 
ratings = pd.concat([ratings1, ratings2],ignore_index=True)
display(ratings)

#* Давайте узнаем количество строк в таблицах ratings и dates, ведь нам
#* предстоит вертикально склеить их между собой:
print('Число строк в таблице ratings: ', ratings.shape[0])
print('Число строк в таблице dates: ', dates.shape[0])
print(ratings.shape[0] == dates.shape[0])
# Число строк в таблице ratings: 100837
# Число строк в таблице dates: 100836
# False
#* Размерность таблиц разная. при выгрузке данных информация об оценках какого-то
#* пользователя попала в обе таблицы (ratings1 и ratings2)
#* В данном примере их легко найти — выведем последнюю строку таблицы ratings1 и первую
#* строку таблицы ratings2:
display(ratings1.tail(1))
display(ratings2.head(1))

#todo Чтобы очистить таблицу от дублей, мы можем воспользоваться методом DataFrame drop_duplicates(),
#todo который удаляет повторяющиеся строки в таблице. Не забываем обновить индексы после удаления дублей,
#todo выставив параметр ignore_index в методе drop_duplicates() на значение True:,
ratings = ratings.drop_duplicates(ignore_index=True)

#* Наконец, мы можем добавить к нашей таблице с оценками даты их выставления. Для этого
#* конкатенируем таблицы ratings и dates по столбцам:

ratings_dates = pd.concat([ratings, dates], axis=1)
display(ratings_dates.tail(7))

#! Объединение DataFrame: join, merge

#* Типы объединений в Pandas тесно связаны с операцией join из SQL, которую мы будем рассматривать
#* в курсе в дальнейшем.

#? inner (внутреннее)
#? При использовании такого типа объединения в результирующей таблице остаются только те записи,
#? которые есть в обеих таблицах. Строки, для которых совпадение не было найдено, удаляются.

#? outer (внешнее). Данный тип делится на три подтипа:

#? full — используется как outer по умолчанию, объединяет все варианты в обеих таблицах.

#? left — для всех записей из «левой» таблицы (например, ratings) ведётся поиск соответствий в
#? «правой» (например, movies). В результирующей таблице останутся только те значения, которым
#? были найдены соответствия, то есть только значения из ratings.

#? right — аналогично предыдущему, но остаются значения только из «правой» таблицы. 

#todo МЕТОД ОБЪЕДИНЕНИЯ JOIN
#* Для объединения двух таблиц по индексам используется метод DataFrame join(). Однако
#* данный метод можно применить и для того, чтобы объединить таблицы по ключевому столбцу

#* other — таблица, которую мы присоединяем. При объединении она является «правой»,
#* а исходная таблица, от имени которой вызывается метод, является «левой».

#* how — параметр типа объединения. Он может принимать значения 'inner', 'left' (left outer), 'right'
#* (right outer), и 'outer' (full outer). По умолчанию параметр установлен на 'left'.

#* on — параметр, который определяет, по какому столбцу в «левой» таблице происходит объединение по индексам из «правой».
#* lsuffix и rsuffix — дополнения (суффиксы) к названиям одноимённых столбцов в «левой» и «правой» таблицах.

#todo Объединим таблицы типом left. Так как в наших таблицах есть одноимённые столбцы,
#todo установим один из суффиксов, чтобы избежать ошибки:
    
joined_false = ratings_dates.join(
    movies,
    rsuffix='_right',
    how='left'
)
display(joined_false)

#* При объединении таблиц по индексам в результирующую таблицу попали все строки из «левой» таблицы,
#* а недостающие строки из «правой» были заполнены пропусками. Так работает тип объединения left.

#todo Чтобы совместить таблицы по ключевому столбцу с помощью метода join(), необходимо использовать
#todo ключевой столбец в «правой» таблице в качестве индекса. Это можно сделать с помощью метода set_index().
#todo Также необходимо указать название ключа в параметре on.

joined = ratings_dates.join(
    movies.set_index('movieId'),
    on='movieId',
    how='left'
)
display(joined.head())

#* В результате такого объединения для каждого идентификатора фильма movieId в таблице ratings_dates
#* найден совпадающий с ним идентификатор movieId в таблице movies и присоединена информация о самом
#* фильме (title и genres). Это как раз то, что нам нужно.

#todo МЕТОД ОБЪЕДИНЕНИЯ MERGE

#? Основные параметры метода merge()
#? right — присоединяемая таблица. По умолчанию она является «правой».

#? how — параметр типа объединения. По умолчанию принимает значение 'inner'.

#? on — параметр, который определяет, по какому столбцу происходит объединение.
#? Определяется автоматически, но рекомендуется указывать вручную.

#? left_on — если названия столбцов в «левой» и «правой» таблицах не совпадают, 
#? то данный параметр отвечает за наименования ключевого столбца исходной таблицы.

#? right_on — аналогично предыдущему, параметр отвечает за наименование ключевого столбца
#? присоединяемой таблицы.

 #* Метод merge() в первую очередь предназначен для слияния таблиц по заданным ключам, поэтому он не
 #* требует установки ключевых столбцов в качестве индекса присоединяемой таблицы. Кроме того, данный
 #* метод позволяет объединять даже таблицы с разноимёнными ключами. Таким образом, merge() проще в
 #* использовании и более многофункционален, чем схожие методы.
 
merged = ratings_dates.merge(
    movies,
    on='movieId',
    how='left'
)
display(merged.head())

 #* Метод merge() с внешним (outer) типом объединения может использоваться как аналог метода concat()
 #* при объединении таблиц с одинаковой структурой (одинаковые количество и названия столбцов) по строкам.
 #* В таком случае все одноимённые столбцы таблиц будут считаться ключевыми.
#todo объединим таблицы ratings1 и ratings2, как мы уже делали раньше, но теперь используем метод merge():
merge_ratings = ratings1.merge(ratings2, how='outer')
print('Число строк в таблице merge_ratings: ', merge_ratings.shape[0])
display(merge_ratings)

 #* при использовании метода merge() для склейки двух таблиц у нас автоматически пропали дубликаты,
 #* которые мы видели при использовании метода concat(). Это особенность метода merge() — 
 #* автоматическое удаление дублей.
 
 
Пример:
 #* Найдите пользователя, который выставил наименьшее количество оценок, но его средняя
 #* оценка фильмам наибольшая.
 
ratings_movies.groupby('userId')['rating'].agg(['count', 'mean']).sort_values(['count', 'mean'], ascending=[True, False])

Пример:
#?  ПРЕДОБРАБОТКА ДАННЫХ
#* Группируем таблицу по дате и названию страны и рассчитываем суммарные показатели по всем регионам. 
#* Тем самым переходим от данных по регионам к данным по странам:

covid_data = covid_data.groupby(
    ['date', 'country'], 
    as_index=False
)[['confirmed', 'deaths', 'recovered']].sum()

#* Преобразуем даты в формат datetime с помощью функции pd.to_datetime():
covid_data['date'] = pd.to_datetime(covid_data['date'])

#* Создадим признак больных на данный момент (active). Для этого вычтем из общего
#* числа зафиксированных случаев число смертей и число выздоровевших пациентов:

covid_data['active'] = covid_data['confirmed'] - covid_data['deaths'] - covid_data['recovered']

#* Создадим признак ежедневного прироста числа заболевших, умерших и выздоровевших людей. Для этого
#* отсортируем данные по названиям стран, а затем по датам. После этого произведём группировку по странам
#* и рассчитаем разницу между «вчера и сегодня» с помощью метода diff():

covid_data = covid_data.sort_values(by=['country', 'date'])
covid_data['daily_confirmed'] = covid_data.groupby('country')['confirmed'].diff()
covid_data['daily_deaths'] = covid_data.groupby('country')['deaths'].diff()
covid_data['daily_recovered'] = covid_data.groupby('country')['recovered'].diff()