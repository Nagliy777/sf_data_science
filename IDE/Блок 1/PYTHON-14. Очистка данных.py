
#! Работа с пропусками: как их обнаружить?

#* Очистка данных (data cleaning) — это процесс обнаружения и удаления (или исправления) повреждённых,
#* ложных или неинформативных записей таблицы или целой базы данных. Процесс состоит из двух этапов:
#* поиск и ликвидация (или редактирование).
#* В pandas пропуски обозначаются специальным символом NaN (Not-a-Number — «не число»). 

#* Найти пропуски зачастую довольно просто за исключением тех случаев, когда пропуски скрыты

#? В библиотеке pandas специально для этого реализован метод isnull().
#* Этот метод возвращает новый DataFrame, в ячейках которого стоят булевы значения True и False.
#* True ставится на месте, где ранее находилось значение NaN.

display(sber_data.isnull().tail())

#* это, мягко говоря, не самый удобный метод поиска пропусков, однако он является промежуточным
#* этапом других способов, которые мы рассмотрим далее.

#? СПИСОК СТОЛБЦОВ С ПРОПУСКАМИ

#* Первый способ — это вывести на экран названия столбцов, где число пропусков больше 0. 
#* Умножаем на 100 %, находим столбцы, где доля пропусков больше 0, сортируем по убыванию и выводим результат:

cols_null_percent = sber_data.isnull().mean() * 100
cols_with_null = cols_null_percent[cols_null_percent>0].sort_values(ascending=False)
display(cols_with_null)

#* Итак, можно увидеть, что у нас большое число пропусков (более 47 %) в столбце hospital_beds_raion
#* (количество больничных коек в округе). Например, уже сейчас ясно, что столбец, в котором почти
#*  половина данных пропущена, не может дать нам полезной информации при прогнозировании.

#? СТОЛБЧАТАЯ ДИАГРАММА ПРОПУСКОВ

#* Иногда столбцов с пропусками становится слишком много и прочитать информацию о них из списка
#* признаков с цифрами становится слишком затруднительно — цифры начинают сливаться воедино.
#* Можно воспользоваться столбчатой диаграммой, чтобы визуально оценить соотношение числа пропусков
#* к числу записей. Самый быстрый способ построить её — использовать метод plot():
    
cols_with_null.plot(
    kind='bar',
    figsize=(10, 4),
    title='Распределение пропусков в данных'
);

#? ТЕПЛОВАЯ КАРТА ПРОПУСКОВ 

#* Её часто используют, когда столбцов с пропусками не так много (меньше 10). Она позволяет понять
#* не только соотношение пропусков в данных, но и их характерное местоположение в таблице. 

#* Для создания такой тепловой карты можно воспользоваться результатом метода isnull(). Ячейки таблицы,
#* в которых есть пропуск, будем отмечать жёлтым цветом, а остальные — синим. Для этого создадим собственную 
#* палитру цветов тепловой карты с помощью метода color_pallete() из библиотеки seaborn.

colors = ['blue', 'yellow'] 
fig = plt.figure(figsize=(10, 4))
cols = cols_with_null.index
ax = sns.heatmap(
    sber_data[cols].isnull(),
    cmap=sns.color_palette(colors),
)

#! МЕТОДЫ ОБРАБОТКИ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ

#* Не существует общего решения проблемы с отсутствующими данными. Для каждого
#* конкретного случая приходится подбирать наиболее подходящие методы или их комбинации.
#* При этом каждая модель уникальна и часто предполагает случайность, поэтому невозможно
#* предсказать заранее, какая комбинация методов сработает на ваших данных лучше всего.

#? ОТБРАСЫВАНИЕ ЗАПИСЕЙ И ПРИЗНАКОВ

#* Первая техника — самая простая из всех. Она предполагает простое удаление записей или признаков,
#* в которых содержатся пропуски. 

#* Здесь важно правильно выбрать ось удаления: если мы избавимся от большого числа строк,
#* то рискуем потерять важные данные, а если мы удалим столбцы, то можем потерять важные признаки.

#* Прежде всего порассуждаем логически: в столбце hospital_beds_raion более 47 % пропусков.
#* Если мы будем удалять все строки, в которых этот признак пропущен, мы потеряем почти половину наших данных! 

#* А вот если мы удалим весь столбец metro_km_walk, где менее 1 % пропусков, то потеряем полезный
#* признак при формировании прогноза цены, ведь расстояние до ближайшего метро — это важный фактор
#* при выборе квартиры. В данном случае лучше будет удалить сами записи.

#todo Специалисты рекомендуют при использовании метода удаления придерживаться следующих правил: 
#todo удаляйте столбец, если число пропусков в нем более 30-40 %. В остальных случаях лучше удалять записи.

#* Для удаления строк и столбцов будем использовать метод dropna(), 
#* который позволяет удалять пропуски с тонким подходом к настройке. 

#* Основные параметры метода:

axis — ось, по которой производится удаление (по умолчанию 0 — строки).
how — как производится удаление строк (any — если хотя бы в одном из столбцов есть пропуск, стоит по умолчанию; all — если во всех столбцах есть пропуски). 
thresh — порог удаления. Определяет минимальное число непустых значений в строке/столбце,
при котором она/он сохраняется. Например, если мы установим thresh в значение 2, то мы удалим строки,
где число пропусков больше чем n-2  и более, где n — число признаков (если axis=0).

#todo Примечание: Начиная с версии 1.5.0 Pandas запрещает одновременно устанавливать параметры how и thresh.

#todo Пример

#* Предварительно создадим копию исходной таблицы — drop_data, чтобы не повредить её. 
#* Зададимся порогом в 70 %: будем оставлять только те столбцы, в которых 70 и более процентов
#* записей не являются пустыми . После этого удалим записи, в которых содержится хотя бы один пропуск.
#* Наконец, выведем информацию о числе пропусков и наслаждаемся нулями. 

#создаем копию исходной таблицы
drop_data = sber_data.copy()
#задаем минимальный порог: вычисляем 70% от числа строк
thresh = drop_data.shape[0]*0.7
#удаляем столбцы, в которых более 30% (100-70) пропусков
drop_data = drop_data.dropna(thresh=thresh, axis=1)
#удаляем записи, в которых есть хотя бы 1 пропуск
drop_data = drop_data.dropna(how='any', axis=0)
#отображаем результирующую долю пропусков
drop_data.isnull().mean()

#* Итак, мы удалили один столбец и ⅓ всех записей для того, чтобы избавиться от пропусков. 
#* Не слишком ли это много и что с этим можно сделать?

#? ЗАПОЛНЕНИЕ НЕДОСТАЮЩИХ ЗНАЧЕНИЙ КОНСТАНТАМИ

#* Чаще всего пустые места заполняют средним/медианой/модой для числовых признаков и модальным
#* значением для категориальных признаков. 
#* Вся сложность заключается в выборе метода заполнения. 
#* Важным фактором при выборе метода является распределение признаков с пропусками. 

#todo Итак, рассмотрим несколько рекомендаций.

#* Для распределений, похожих на логнормальное, где пик близ нуля, а далее наблюдается постепенный
#* спад частоты, высока вероятность наличия выбросов (о них мы поговорим чуть позже). Математически
#* доказывается, что среднее очень чувствительно к выбросам, а вот медиана — нет. Поэтому предпочтительнее
#* использовать медианное значение для таких признаков.

#* Если признак числовой и дискретный (например, число этажей, школьная квота), то их заполнение
#* средним/медианой является ошибочным, так как может получиться число, которое не может являться
#* значением этого признака. Например, количество этажей — целочисленный признак, а расчёт среднего
#* может дать 2.871. Поэтому такой признак заполняют либо модой, либо округляют до целого числа (или
#* нужного количества знаков после запятой) среднее/медиану.

#* Категориальные признаки заполняются либо модальным значением, либо, если вы хотите оставить информацию
#* о пропуске в данных, значением 'unknown'. На наше счастье, пропусков в категориях у нас нет.

#* Иногда в данных бывает такой признак, основываясь на котором, можно заполнить пропуски в другом. 
#* Например, в наших данных есть признак full_sq (общая площадь квартиры). Давайте исходить из
#* предположения, что, если жилая площадь (life_sq) неизвестна, то она будет равна суммарной площади!

#* Заполнение значений осуществляется с помощью метода fillna(). Главный параметр метода — value
#* (значение, на которое происходит заполнение данных в столбце). 

#todo Пример
#* Создадим такой словарь, соблюдая рекомендации, приведённые выше, а также копию исходной таблицы.
#* Произведём операцию заполнения с помощью метода fillna() и удостоверимся, что пропусков в данных
#* больше нет:

#создаем копию исходной таблицы
fill_data = sber_data.copy()
#создаем словарь имя столбца: число(признак) на который надо заменить пропуски
values = {
    'life_sq': fill_data['full_sq'],
    'metro_min_walk': fill_data['metro_min_walk'].median(),
    'metro_km_walk': fill_data['metro_km_walk'].median(),
    'railroad_station_walk_km': fill_data['railroad_station_walk_km'].median(),
    'railroad_station_walk_min': fill_data['railroad_station_walk_min'].median(),
    'hospital_beds_raion': fill_data['hospital_beds_raion'].mode()[0],
    'preschool_quota': fill_data['preschool_quota'].mode()[0],
    'school_quota': fill_data['school_quota'].mode()[0],
    'floor': fill_data['floor'].mode()[0]
}
#заполняем пропуски в соответствии с заявленным словарем
fill_data = fill_data.fillna(values)
#выводим результирующую долю пропусков
fill_data.isnull().mean()

#* Недостаток метода заполнения константой состоит в том, что мы можем «нафантазировать» новые данные,
#* которые не учитывают истинного распределения.

#? ЗАПОЛНЕНИЕ НЕДОСТАЮЩИХ ЗНАЧЕНИЙ КОНСТАНТАМИ С ДОБАВЛЕНИЕМ ИНДИКАТОРА

#* Если мы используем заполнение пропусков константами, может быть, имеет смысл
#* сказать модели о том, что на этом месте был пропуск? 
#* Давайте добавим к нашим данным признаки-индикаторы, которые будут сигнализировать о том, что в столбце
#* на определённом месте в таблице был пропуск. Это место в столбце-индикаторе будем помечать как True. 

#todo Пример

#* Посмотрим на реализацию. Как обычно, создадим копию indicator_data исходной таблицы. 
#* В цикле пройдёмся по столбцам с пропусками и будем добавлять в таблицу новый признак (с припиской 
#* "was_null"), который получается из исходного с помощью применения метода isnull(). После чего
#* произведём обычное заполнение пропусков, которое мы совершали ранее, и выведем на экран число
#* отсутствующих значений в столбце, чтобы убедиться в результате:

#создаем копию исходной таблицы
indicator_data = sber_data.copy()
#в цикле пробегаемся по названиям столбцов с пропусками
for col in cols_with_null.index:
    #создаем новый признак-индикатор как col_was_null
    indicator_data[col + '_was_null'] = indicator_data[col].isnull()
#создаем словарь имя столбца: число(признак) на который надо заменить пропуски   
values = {
    'life_sq': indicator_data['full_sq'],
    'metro_min_walk': indicator_data['metro_min_walk'].median(),
    'metro_km_walk': indicator_data['metro_km_walk'].median(),
    'railroad_station_walk_km': indicator_data['railroad_station_walk_km'].median(),
    'railroad_station_walk_min': indicator_data['railroad_station_walk_min'].median(),
    'hospital_beds_raion': indicator_data['hospital_beds_raion'].mode()[0],
    'preschool_quota': indicator_data['preschool_quota'].mode()[0],
    'school_quota': indicator_data['school_quota'].mode()[0],
    'floor': indicator_data['floor'].mode()[0]
}
#заполняем пропуски в соответствии с заявленным словарем
indicator_data = indicator_data.fillna(values)
#выводим результирующую долю пропусков
indicator_data.isnull().mean()

#todo Метод исходит из предположения, что, если дать модели информацию о том, что в ячейке ранее была
#todo пустота, то она будет меньше доверять таким записям и меньше учитывать её в процессе обучения.
#todo Иногда такие фишки действительно работают, иногда не дают эффекта, а иногда и вовсе могут ухудшить
#todo результат обучения и затруднить процесс обучения.

#* Недостаток размерности гласит, что, увеличивая размерность функции, мы повышаем сложность
#* поиска этого минимума и рискуем вовсе не найти его! Об этом страшном проклятии мы ещё будем
#* говорить в курсе по ML и даже попробуем его победить. Но об этом чуть позже.

#todo Однако, несмотря на свои недостатки, этот метод кажется наиболее логичным из предложенных
#todo ранее и часто используется в очистке данных.

#? КОМБИНИРОВАНИЕ МЕТОДОВ

#* Наверняка вы уже догадались, что необязательно использовать один метод. Вы можете их комбинировать.
#* Например, мы можем:

#todo удалить столбцы, в которых более 30 % пропусков;
#todo удалить записи, в которых более двух пропусков одновременно;
#todo заполнить оставшиеся ячейки константами.

#todo Посмотрим на реализацию такого подхода в коде:

#создаём копию исходной таблицы
combine_data = sber_data.copy()

#отбрасываем столбцы с числом пропусков более 30% (100-70)
n = combine_data.shape[0] #число строк в таблице
thresh = n*0.7
combine_data = combine_data.dropna(thresh=thresh, axis=1)

#отбрасываем строки с числом пропусков более 2 в строке
m = combine_data.shape[1] #число признаков после удаления столбцов
combine_data = combine_data.dropna(thresh=m-2, axis=0)

#создаём словарь 'имя_столбца': число (признак), на который надо заменить пропуски 
values = {
    'life_sq': combine_data['full_sq'],
    'metro_min_walk': combine_data['metro_min_walk'].median(),
    'metro_km_walk': combine_data['metro_km_walk'].median(),
    'railroad_station_walk_km': combine_data['railroad_station_walk_km'].median(),
    'railroad_station_walk_min': combine_data['railroad_station_walk_min'].median(),
    'preschool_quota': combine_data['preschool_quota'].mode()[0],
    'school_quota': combine_data['school_quota'].mode()[0],
    'floor': combine_data['floor'].mode()[0]
}
#заполняем оставшиеся записи константами в соответствии со словарем values
combine_data = combine_data.fillna(values)
#выводим результирующую долю пропусков
display(combine_data.isnull().mean())

#todo Вы можете придумывать свои собственные комбинации методов и использовать их для борьбы с пропусками,
#todo главное — найти баланс между потерей информации и её искажением.

#todo Вы можете оставить пропуски как есть, просто заменив их на какой-то специальный символ. Например,
#todo для числовых неотрицательных признаков можно использовать число -1, а для категориальных — строку 'unknown'.

#! Выбросы: почему появляются и чем опасны?

#* Выброс (аномалия) — это наблюдение, которое существенно выбивается из общего распределения
#* и сильно отличается от других данных.

#* Так или иначе, проблема выбросов состоит в том, что они могут «шокировать» модель. 

#* Модели — это математические методы, которые оперируют числами и пытаются подстроить зависимости
#* в данных, чтобы выдать верный ответ. Когда модель сталкивается с выбросом, она пытается подстроиться
#* и под него. В результате зависимость искажается, качество моделирования падает. Иногда выбросов может
#* быть так много, что модель и вовсе начинает выдавать какую-то белиберду. Такие модели, конечно, никому
#* не нужны, поэтому специалисты особенно озадачены поиском аномалий в данных.

#todo Чаще всего выбросы либо исключают из данных, либо корректируют значения на что-то правдоподобное,
#todo либо, если выбросов много, создают специальный датасет и обучают модели для них отдельно.

#todo ПРИЧИНЫ ПОЯВЛЕНИЯ ВЫБРОСОВ

#* Ошибка ввода данных. Как и с пропусками, здесь играет роль человеческий фактор. Ввести лишний «ноль»
#* на клавиатуре и не заметить этого — нередкий случай.

#* Ошибки отбора выборки. Когда начинающие специалисты составляют обучающую выборку для моделирования,
#* они часто допускают ошибку, не выделив в отдельную группу часть данных. Типичный пример: смешать
#* вместе данные об уровне жизни среднего класса, опрошенного на улице, и добавить туда людей из списка
#* Forbes.

#* Преднамеренное искажение или мошенничество. Пользователи приложения часто намеренно указывают неверные
#* данные о себе. Например, прибавляют к своему возрасту лишнюю сотню лет, создавая головную боль
#* дата-сайентистам.

#! Методы выявления выбросов

#? МЕТОД РУЧНОГО ПОИСКА И ЗДРАВОГО СМЫСЛА

#todo Это самый трудоёмкий метод, основанный на житейской логике, методе пристального взгляда
#todo и небольшом количестве статистики. Он предполагает поиск невозможных и нелогичных значений
#todo в данных.

#* Пусть у нас есть признак, по которому мы будем искать выбросы. Давайте рассчитаем его статистические
#* показатели (минимум, максимум, среднее, квантили) и по ним попробуем определить наличие аномалий.
#* Сделать это можно с помощью уже знакомого вам метода describe()

sber_data['life_sq'].describe()

#* Что нам говорит метод describe()? Во-первых, у нас есть квартиры с нулевой жилой площадью.
#* Во-вторых, в то время как 75-й квантиль равен 43, максимум превышает 7 тысяч квадратных метров
#* (целый дворец, а не квартира!). 

#todo Найдём число квартир с нулевой жилой площадью:
print(sber_data[sber_data['life_sq'] == 0].shape[0])

#* Это могут быть апартаменты — вид коммерческой недвижимости, которая юридически
#* не является жилой площадью, 

#* А теперь выведем здания с жилой площадью более 7 000 квадратных метров:
display(sber_data[sber_data['life_sq'] > 7000])

#* Выброс налицо: гигантская жилая площадь (life_sq), да ещё почти в 100 раз превышает
#* общую площадь (full_sq). Логичен вопрос: а много ли у нас таких квартир, у которых жилая
#* площадь больше, чем суммарная?
outliers = sber_data[sber_data['life_sq'] > sber_data['full_sq']]
print(outliers.shape[0])

#* Таких квартир оказывается 37 штук. Подобные наблюдения уже не поддаются здравому смыслу
#* — они являются ошибочными, и от них стоит избавиться. Для этого можно воспользоваться методом
#* drop() и удалить записи по их индексам:
cleaned = sber_data.drop(outliers.index, axis=0)
print(f'Результирующее число записей: {cleaned.shape[0]}')

#? МЕТОД МЕЖКВАРТИЛЬНОГО РАЗМАХА (МЕТОД ТЬЮКИ)

#* Отличным помощником в поиске потенциальных выбросов является визуализация. Если признак является
#* числовым, то можно построить гистограмму или коробчатую диаграмму, чтобы найти аномалии.

#todo На гистограмме мы можем увидеть потенциальные выбросы как низкие далеко отстоящие от основной группы
#todo столбцов «пеньки», а на коробчатой диаграмме — точки за пределами усов.
#* Построим гистограмму и коробчатую диаграмму для признака полной площади (full_sq):

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))
histplot = sns.histplot(data=sber_data, x='full_sq', ax=axes[0]);
histplot.set_title('Full Square Distribution');
boxplot = sns.boxplot(data=sber_data, x='full_sq', ax=axes[1]);
boxplot.set_title('Full Square Boxplot');

Алгоритм метода:

→ вычислить 25-ый и 75-ый квантили (первый и третий квартили) — Q25 и Q75 для признака, который мы исследуем;

→ вычислить межквартильное расстояние:IQR = Q75 - Q25;

→ вычислить верхнюю и нижнюю границы Тьюки: 
bound(lower)=Q25 - 1.5 * IQR
bound(upper)=Q75 + 1.5 * IQR

найти наблюдения, которые выходят за пределы границ [bound(lower) и bound(upper)]

#todo Пример

#* В соответствии с этим алгоритмом напишем функцию outliers_iqr(), которая вам может ещё не
#* раз пригодиться в реальных задачах. Эта функция принимает на вход DataFrame и признак, по
#* которому ищутся выбросы, а затем возвращает потенциальные выбросы, найденные с помощью метода
#* Тьюки, и очищенный от них датасет.
#* Квантили вычисляются с помощью метода quantile(). 

def outliers_iqr(data, feature):
    x = data[feature]
    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),
    iqr = quartile_3 - quartile_1
    lower_bound = quartile_1 - (iqr * 1.5)
    upper_bound = quartile_3 + (iqr * 1.5)
    outliers = data[(x < lower_bound) | (x > upper_bound)]
    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]
    return outliers, cleaned

 #todo Применим эту функцию к таблице sber_data и признаку full_sq, а также выведем размерности результатов:
outliers, cleaned = outliers_iqr(sber_data, 'full_sq')
print(f'Число выбросов по методу Тьюки: {outliers.shape[0]}')
print(f'Результирующее число записей: {cleaned.shape[0]}')

#todo Классический метод межквартильного размаха не учитывает особенностей распределения! Он требует,
#todo чтобы данные были распределены плюс-минус нормально (гистограмма должна быть похожа на колокол)
#todo и требует от распределения примерной симметричности (чтобы у гистограммы были одинаковые хвосты
#todo в обе стороны).

#* У нас же распределение (даже после отсечения выбросов) отличается от заявленных критериев.
#* Оно несимметрично: правый хвост изначального распределения гораздо длиннее левого (для Москвы
#* вполне естественны квартиры с площадью свыше 100 квадратных метров) и вовсе не колоколообразно.
#* Попросту говоря, выбор метода поиска не оправдал себя.

#todo Вы можете сами подбирать число размахов влево и/или вправо и таким образом отбирать выбросы,
#todo учитывая особенности ваших данных. 


#* Давайте немного модифицируем функцию outliers_iqr(data, feature). Добавьте в неё параметры
#* left и right, которые задают число IQR влево и вправо от границ ящика (пусть по умолчанию они равны 1.5)

def outliers_iqr_mod(data, feature, left=1.5, right=1.5):
    x = data[feature]
    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),
    iqr = quartile_3 - quartile_1
    lower_bound = quartile_1 - (iqr * left)
    upper_bound = quartile_3 + (iqr * right)
    outliers = data[(x < lower_bound) | (x > upper_bound)]
    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]
    return outliers, cleaned


#? МЕТОД Z-ОТКЛОНЕНИЙ (МЕТОД СИГМ)

#todo Правило трёх сигм гласит: если распределение данных является нормальным, то 99,73 %
#todo лежат в интервале от(m-3q, m+3q), где  m(мю) — математическое ожидание (для выборки это среднее 
#todo  значение), а  q(сигма) — стандартное отклонение. Наблюдения, которые лежат за пределами этого интервала,
#todo будут считаться выбросами.

#* А что делать, если данные не распределены нормально? 

#* На такой случай есть один трюк. Иногда для распределений, похожих на логнормальное,
#* может помочь логарифмирование. Оно может привести исходное распределение к подобию нормального.
#* Причем, основание логарифма может быть любым.

#todo Рассмотрим логарифмирование на примере. 

#* Построим две гистограммы признака расстояния до МКАД (mkad_km): первая — в обычном масштабе,
#* а вторая — в логарифмическом. Логарифмировать будем с помощью функции log()
#* из библиотеки numpy (натуральный логарифм — логарифм по основанию числа e). Признак имеет
#* среди своих значений 0. Из математики известно, что логарифма от 0 не существует, поэтому мы
#* прибавляем к нашему признаку 1, чтобы не логарифмировать нули и не получать предупреждения.

fig, axes = plt.subplots(1, 2, figsize=(15, 4))

#гистограмма исходного признака
histplot = sns.histplot(sber_data['mkad_km'], bins=30, ax=axes[0])
histplot.set_title('MKAD Km Distribution');

#гистограмма в логарифмическом масштабе
log_mkad_km= np.log(sber_data['mkad_km'] + 1)
histplot = sns.histplot(log_mkad_km , bins=30, ax=axes[1])
histplot.set_title('Log MKAD Km Distribution');

#* Взяв натуральный логарифм от левого распределения, мы получаем правое, которое напоминает
#* слегка перекошенное нормальное. Слева от моды (самого высокого столбика) наблюдается чуть больше
#* наблюдений, нежели справа. По-научному это будет звучать так: «распределение имеет левостороннюю асимметрию».

#todo Примечание: Численный показатель асимметрии можно вычислить с помощью метода:
skew():
print(log_mkad_km.skew())
# -0.14263612203024953
#* Асимметрия распределения называется правосторонней, если она положительная:
As>0:
#* Асимметрия распределения называется левосторонней, если она отрицательная:
As<0

#todo Давайте реализуем алгоритм метода z-отклонения. Описание алгоритма метода:

#* → вычислить математическое ожидание m(мю) (среднее) и стандартное отклонение q(сигма) признака x;

#* → вычислить нижнюю и верхнюю границу интервала как:
#* bound(lower)=m - 3q
#* bound(upper)=m + 3q
#* → найти наблюдения, которые выходят за пределы границ.

#todo Напишем функцию outliers_z_score(), которая реализует этот алгоритм. 

#* На вход она принимает DataFrame и признак, по которому ищутся выбросы. В дополнение добавим
#* в функцию возможность работы в логарифмическом масштабе: для этого введём аргумент log_scale.
#* Если он равен True, то будем логарифмировать рассматриваемый признак, иначе — оставляем его
#* в исходном виде.

def outliers_z_score(data, feature, log_scale=False):
    if log_scale:
        x = np.log(data[feature]+1)
    else:
        x = data[feature]
    mu = x.mean()
    sigma = x.std()
    lower_bound = mu - 3 * sigma
    upper_bound = mu + 3 * sigma
    outliers = data[(x < lower_bound) | (x > upper_bound)]
    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]
    return outliers, cleaned

outliers, cleaned = outliers_z_score(sber_data, 'mkad_km', log_scale=True)
print(f'Число выбросов по методу z-отклонения: {outliers.shape[0]}')
print(f'Результирующее число записей: {cleaned.shape[0]}')

#* Итак, метод z-отклонения нашел нам 33 потенциальных выброса по признаку расстояния до МКАД.
#* Давайте узнаем, в каких районах (sub_area) представлены эти квартиры:
print(outliers['sub_area'].unique())

#* Наши потенциальные выбросы — это квартиры из поселений «Роговское» и «Киевский».
#* Снова обращаемся к силе интернета и «пробиваем» наших подозреваемых. Эти поселения —
#* самые удалённые районы Москвы; первое из них — это и вовсе граница с Калужской областью. 

#todo И тут возникает закономерный вопрос: а стоит ли считать такие наблюдения за выбросы? 
#* Вопрос в действительности не имеет определенного ответа: с одной стороны, метод прямо-таки
#* говорит нам об этом, а с другой — эти наблюдения имеют право на существование, ведь они
#* являются частью Москвы.

#todo Возможно, мы не учли того факта, что наш логарифм распределения всё-таки не идеально нормален и в
#todo нём присутствует некоторая асимметрия. Возможно, стоит дать некоторое «послабление» на границы
#todo интервалов? Давайте отдельно построим гистограмму прологарифмированного распределения, а также
#todo отобразим на гистограмме вертикальные линии, соответствующие среднему (центру интервала в методе
#todo трёх сигм) и границы интервала m+3q. Вертикальные линии можно построить с помощью метода axvline().
#todo  Для среднего линия будет обычной, а для границ интервала — пунктирной (параметр ls ='--'):

fig, ax = plt.subplots(1, 1, figsize=(8, 4))
log_mkad_km = np.log(sber_data['mkad_km'] + 1)
histplot = sns.histplot(log_mkad_km, bins=30, ax=ax)
histplot.axvline(log_mkad_km.mean(), color='k', lw=2)
histplot.axvline(log_mkad_km.mean()+ 3 * log_mkad_km.std(), color='k', ls='--', lw=2)
histplot.axvline(log_mkad_km.mean()- 3 * log_mkad_km.std(), color='k', ls='--', lw=2)
histplot.set_title('Log MKAD Km Distribution');

#todo Проверьте, что будет, если дать «послабление» вправо, увеличив число сигм. Наша задача — узнать,
#todo начиная с какой границы поселения «Роговское» и «Киевское» перестают считаться выбросами.

def outliers_z_score_mod(data, feature, log_scale=False, left=3, right=3):
    if log_scale:
        x = np.log(data[feature]+1)
    else:
        x = data[feature]
    mu = x.mean()
    sigma = x.std()
    lower_bound = mu - left * sigma 
    upper_bound = mu + right * sigma 
    outliers = data[(x < lower_bound) | (x > upper_bound)]
    cleaned = data[(x >= lower_bound) & (x <= upper_bound)]
    return outliers, cleaned

outliers,cleaned = outliers_z_score_mod(sber_data, 'mkad_km', log_scale=True, left=3, right=3.5)
print(f'Число выбросов по методу z-отклонения: {outliers.shape[0]}')
print(f'Результирующее число записей: {cleaned.shape[0]}')


#* Стоило нам немного увеличить правую границу метода z-отклонений, как мы получили отсутствие выбросов
#* в признаке. Давать ли такие «послабления» или нет — это ваше решение и полностью зависит от специфики
#* задачи, однако вы можете пользоваться этим трюком, чтобы более тщательно подходить к поиску аномалий.

#todo РЕЗЮМЕ ПО МЕТОДАМ ПОИСКА ВЫБРОСОВ

#* Мы рассмотрели классические методы выявления аномальных данных. Как вы сами поняли, каждый из них
#* имеет свои преимущества и недостатки.

#* Метод ручного поиска тяжело автоматизировать, однако вы сами организовываете проверки, следите
#* за процессом отсеивания выбросов и руководствуетесь не только статистикой, но и здравым смыслом.
#* К тому же из всех представленных только этот метод способен выявить логические нестыковки в данных
#* (например, общая площадь меньше жилой или число этажей более 77 в районе, где нет таких зданий).
#* Обычно этот метод используется в комплексе с другими, чтобы удостовериться в том, что найденные 
#* данные действительно являются выбросами, и произвести логическую очистку, на которую неспособны 
#* другие методы.

#* Методы межквартильного размаха и z-отклонений довольно просты в реализации и основаны на стройной
#* математической теории, а не на эвристиках. Но это их преимущество и недостаток. Оба метода разработаны
#* для данных, распределённых приблизительно нормально. Проблема в том, что далеко не всё в мире имеет
#* нормальное распределение. Необходимо внимательно «рыться» в данных, совершать дополнительные
#* преобразования,чтобы привести их хотя бы к подобию нормального распределения, либо подбирать
#* границы методов, чтобы учитывать особенности распределений. В противном случае методы начинают
#* считать выбросами всё, что не вписывается в жёсткие границы, и вы рискуете лишиться важных данных!
#* Учитывайте это в своей работе и всегда проверяйте результаты.

#! Работа с дубликатами и неинформативными признаками

#? ДУБЛИКАТЫ

#* Дубликатами называются записи, для которых значения (всех или большинства) признаков совпадают. 
#todo Дублирующаяся информация никогда не приводит ни к чему хорошему. В Data Science одинаковые записи
#todo не несут полезной информации и искажают реальную статистику. Модель несколько раз видит одно и то
#todo же наблюдение и начинает подстраиваться под него. Если дубликатов много, это может стать большой
#todo проблемой при обучении.

#* ОБНАРУЖЕНИЕ И ЛИКВИДАЦИЯ ДУБЛИКАТОВ

#todo Чтобы отследить дубликаты, можно воспользоваться методом duplicated(),
#todo который возвращает булеву маску для фильтрации. Для записей, у которых
#todo совпадают признаки, переданные методу, он возвращает True, для остальных — False.
#todo У метода есть параметр subset — список признаков, по которым производится поиск дубликатов.
#todo По умолчанию используются все столбцы в DataFrame и ищутся полные дубликаты.

#todo Пример
#* Создадим маску дубликатов с помощью метода duplicated() и произведём фильтрацию. Результат заносим
#* в переменную sber_duplicates. Выведем число строк в результирующем DataFrame:

dupl_columns = list(sber_data.columns)
dupl_columns.remove('id')

mask = sber_data.duplicated(subset=dupl_columns)
sber_duplicates = sber_data[mask]
print(f'Число найденных дубликатов: {sber_duplicates.shape[0]}')
# Число найденных дубликатов: 562

#* Теперь нам необходимо от них избавиться. Для этого легче всего воспользоваться методом drop_duplicates(),
#* который удаляет повторяющиеся записи из таблицы. 

#* Создадим новую таблицу sber_dedupped, которая будет версией исходной таблицы,
#* очищенной от полных дубликатов.

sber_dedupped = sber_data.drop_duplicates(subset=dupl_columns)
print(f'Результирующее число записей: {sber_dedupped.shape[0]}')

#? НЕИНФОРМАТИВНЫЕ ПРИЗНАКИ

#* Неинформативными называются признаки, в которых большая часть строк содержит одинаковые значения
#* (например, пол клиентов в мужском барбершопе), либо наоборот — признак, в котором для большинства
#* записей значения уникальны (например, номер телефона клиента). 

#todo Такие признаки не играют роли при моделировании и лишь засоряют таблицу, увеличивая размерность данных.
#todo Они усиливают уже знакомое нам проклятие размерности, которое увеличивает время обучения модели
#todo и потенциально может снизить ее качество. Поэтому от таких признаков необходимо безжалостно избавляться.

#* Чтобы считать признак неинформативным, прежде всего нужно задать какой-то определённый порог. Например,
#* часто используют пороги в 0.95 и 0.99. Это означает: признак неинформативен, если в нем 95 % (99 %)
#* одинаковых значений или же 95 % (99 %) данных полностью уникальны. 

#todo Разберём алгоритм:

#* → Создаём пустой список low_information_cols, куда будем добавлять названия признаков, 
#* которые мы посчитаем неинформативными.

#* → В цикле пройдёмся по всем именам столбцов в таблице и для каждого будем совершать следующие действия:

#* Рассчитаем top_freq — наибольшую относительную частоту с помощью метода value_counts() с
#* параметром normalize=True. Метод вернёт долю от общих данных, которую занимает каждое уникальное
#* значение в признаке.

#* Рассчитаем nunique_ratio — отношение числа уникальных значений в столбце к размеру всего столбца.
#* Число уникальных значений в столбце получим с помощью метода nunique(), а размер признака — с помощью 
#* метода count(). Например, для столбца id число уникальных значений — 30471; оно же равно размеру таблицы.
#* Поэтому результат отношения будет 1.

#* Сравним каждое из полученных чисел с пороговым значением (у нас это 0.95) и добавим в список 
#* неинформативных признаков, если условие истинно.

#todo Пример

#список неинформативных признаков
low_information_cols = [] 

#цикл по всем столбцам
for col in sber_data.columns:
    #наибольшая относительная частота в признаке
    top_freq = sber_data[col].value_counts(normalize=True).max()
    #доля уникальных значений от размера признака
    nunique_ratio = sber_data[col].nunique() / sber_data[col].count()
    # сравниваем наибольшую частоту с порогом
    if top_freq > 0.95:
        low_information_cols.append(col)
        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')
    # сравниваем долю уникальных значений с порогом
    if nunique_ratio > 0.95:
        low_information_cols.append(col)
        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')
        
#* Итак, мы нашли шесть неинформативных признаков. Теперь можно удалить их с помощью метода drop(),
#* передав результирующий список в его аргументы.

information_sber_data = sber_data.drop(low_information_cols, axis=1)
print(f'Результирующее число признаков: {information_sber_data.shape[1]}')

#todo Однако всегда следует скептически относиться к результатам, которые предоставил алгоритм поиска
#todo неинформативных признаков. В противном случае можно лишиться важных данных. 

#todo Будьте внимательны и рассудительны при поиске неинформативных признаков. Лучшее 
#todo решение — для начала использовать все признаки для построения базовой модели, а
#todo затем уже выбирать те, которые обладают наибольшей информативностью.

#? ВАЖНОСТЬ ПРИЗНАКОВ

#todo На самом деле информативность признаков определяется не только числом уникальных значений,
#todo но и их влиянием на целевой признак (тот, который мы хотим предсказать). Это называется
#todo важностью признака. 

#* Признаки, которые обладают низкой важностью, называют нерелевантными признаками. 

